# ğŸ§© miniGPT â€” minimalny model jÄ™zykowy w Pythonie

Ten projekt to **edukacyjna implementacja GPT** napisana w czystym Pythonie + PyTorch.  
Celem jest **zrozumienie kaÅ¼dego elementu transformera**, od embeddingÃ³w po generacjÄ™ tekstu.

---

## ğŸ“ Struktura projektu

```
mini_gpt/
 â”œâ”€â”€ __init__.py
 â”œâ”€â”€ tokenizer_char.py         # prosty tokenizer znakowy
 â””â”€â”€ model/
     â”œâ”€â”€ __init__.py
     â”œâ”€â”€ config.py             # konfiguracja modelu (rozmiary, warstwy)
     â”œâ”€â”€ token_embedding.py    # zamiana indeksÃ³w tokenÃ³w na wektory
     â”œâ”€â”€ positional_embedding.py  # dodaje informacjÄ™ o pozycji
     â”œâ”€â”€ multi_head_attention.py  # rdzeÅ„ transformera (self-attention)
     â”œâ”€â”€ feed_forward.py       # sieÄ‡ MLP po attention
     â”œâ”€â”€ block.py              # pojedynczy blok transformera
     â””â”€â”€ gpt.py                # peÅ‚ny model MiniGPT z generate() i compute_loss()

tests/
 â”œâ”€â”€ test_shapes.py            # sprawdza poprawnoÅ›Ä‡ wymiarÃ³w
 â”œâ”€â”€ run_generate.py           # generuje przykÅ‚adowy tekst
 â””â”€â”€ train_step.py             # prosty trening (opcjonalny)
```

---

## âš™ï¸ PrzepÅ‚yw danych

```
idx (B,T)
  â”‚
  â–¼
TokenEmbedding (B,T,d_model)
  â”‚
  â–¼
PositionalEmbedding
  â”‚
  â–¼
N Ã— [LayerNorm â†’ MultiHeadAttention â†’ +residual â†’ LayerNorm â†’ FeedForward â†’ +residual]
  â”‚
  â–¼
LayerNorm
  â”‚
  â–¼
Linear (lm_head)
  â”‚
  â–¼
logits (B,T,vocab)
```

---

## ğŸ§  Diagramy

### a) Attention z maskÄ… przyczynowÄ…

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Input x â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Linear(3*d_model) â†’ [Q,K,V]â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  podziaÅ‚ na    â”‚
                    â”‚  n_head gÅ‚Ã³w   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                  â–¼                  â–¼
         Qâ‚                 Qâ‚‚                 Qâ‚™
         Kâ‚                 Kâ‚‚                 Kâ‚™
         Vâ‚                 Vâ‚‚                 Vâ‚™
          â”‚                  â”‚                  â”‚
          â–¼                  â–¼                  â–¼
        QKáµ€/âˆšd âŸ¶ mask upper-tri  (zero future)
          â”‚
          â–¼
      softmax(attention weights)
          â”‚
          â–¼
        AÂ·V â†’ gÅ‚owa wynikowa
          â”‚
          â–¼
    concat wszystkich gÅ‚Ã³w
          â”‚
          â–¼
   Linear projection â†’ (B,T,d_model)
          â”‚
          â–¼
   Dropout + Residual + LayerNorm
```

â¡ï¸ Maska trÃ³jkÄ…tna (`triu`) gwarantuje, Å¼e token *t* nie widzi tokenÃ³w *t+1â€¦T*.

---

### b) Shift w `compute_loss()`

```
WejÅ›cie:  idx = [h, e, l, l, o]
Logits:   model przewiduje prawdopodobieÅ„stwo KAÅ»DEGO tokenu
          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
          â”‚hâ†’?â”‚eâ†’?â”‚lâ†’?â”‚lâ†’?â”‚oâ†’?â”‚
          â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Targets:  przesuniÄ™cie o 1 w lewo
          [e, l, l, o, <pad>]

Loss liczymy miÄ™dzy:
    logits[:, :-1, :]  vs  targets[:, 1:]
czyli model uczy siÄ™ przewidywaÄ‡ NASTÄ˜PNY znak
```

---

## ğŸš€ Uruchomienie

```bash
pip install torch
python -m tests.test_shapes
python -m tests.run_generate
```

*(opcjonalnie)* trening:
```bash
python -m tests.train_step
```

---

## ğŸ¯ Cel projektu

- ZrozumieÄ‡ **wewnÄ™trznÄ… logikÄ™ transformera GPT**.  
- MieÄ‡ wÅ‚asny, w peÅ‚ni czytelny kod od embeddingÃ³w po sampling.  
- MÃ³c potem samodzielnie rozwijaÄ‡ model (RAG, BPE, funkcje, itp.).

---

## ğŸ“š Autor

Projekt edukacyjny Macieja â€” nauka LLM â€od Å›rodkaâ€.
